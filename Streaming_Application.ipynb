{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether same location or not\n",
    "def get_geohash(lat1,long1,lat2,long2):\n",
    "    return geohash.encode(lat1,long1, precision=5) == geohash.encode(lat2,long2, precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Mongo DB into assignment db and stream collection\n",
    "def save_to_db(dictionary):\n",
    "    client = MongoClient()\n",
    "    db = client.assignment\n",
    "    stream = db.stream\n",
    "    stream.insert_one(dictionary)\n",
    "    client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_all_stream(rdd):\n",
    "    # Turn RDD into iterable\n",
    "    iter = rdd.collect() \n",
    "    \n",
    "    # Lists to store hotspot and climate entries\n",
    "    hotspot_list = []\n",
    "    climate_list = []\n",
    "    \n",
    "    # For every delivered record...\n",
    "    for record in iter:\n",
    "        ## ...Ignore None part of tuple\n",
    "        data = record[1]\n",
    "        \n",
    "        ## ...Turn into json format\n",
    "        data=json.loads(data)\n",
    "        \n",
    "        ## ...Get sender ID\n",
    "        sender=data['sender_id']\n",
    "        \n",
    "        ## ...If climate data...\n",
    "        if sender==1:\n",
    "            ###...... Add to climate list\n",
    "            climate_list.append(data)\n",
    "\n",
    "        ##... If not\n",
    "        else:\n",
    "            ###...... Add to hotspot list\n",
    "            hotspot_list.append(data)\n",
    "            \n",
    "    \n",
    "    # If no hotspot record recieved...\n",
    "    if len(hotspot_list)==0:\n",
    "        ## ... For every entry in climate list...\n",
    "        for document in climate_list:\n",
    "            ###...Directly save to Database\n",
    "            save_to_db(document)\n",
    "            \n",
    "    # If 1 hotspot record recieved...       \n",
    "    elif len(hotspot_list)==1:\n",
    "        ## ...For every entry in climate list...\n",
    "        for document in climate_list:\n",
    "            hot_spot_doc = hotspot_list[0]\n",
    "            \n",
    "            ###....Check whether hotspot and climate location same...\n",
    "            if get_geohash(document['latitude'],document['longitude']\n",
    "                        ,hot_spot_doc['latitude'],hot_spot_doc['longitude']):\n",
    "                \n",
    "                ####... If yes, embed hotspot into climate record\n",
    "                document['hotspot']= hot_spot_doc\n",
    "                ####... save to database\n",
    "                save_to_db(document)\n",
    "            ###... If location is not matching  \n",
    "            else:\n",
    "                ####... save only the climate record to database\n",
    "                save_to_db(document)\n",
    "    \n",
    "    # If 2 hotspot records recieved...\n",
    "    elif len(hotspot_list)==2:\n",
    "        hot_spot_doc1=hotspot_list[0]\n",
    "        hot_spot_doc2=hotspot_list[1]    \n",
    "        \n",
    "        ## ...For every entry in climate list...\n",
    "        for document in climate_list:\n",
    "            \n",
    "            ###....Check whether climate location and 2 of hotspor location same...\n",
    "            if (get_geohash(document[\"latitude\"], document[\"longitude\"]\n",
    "                           ,hot_spot_doc1[\"latitude\"], hot_spot_doc1[\"longitude\"]) and get_geohash(document[\"latitude\"]\n",
    "                                                                                                   , document[\"longitude\"],hot_spot_doc2[\"latitude\"], hot_spot_doc2[\"longitude\"])):\n",
    "                \n",
    "                ####.... Take average of confidence and surface temperature of hotspot records\n",
    "                hot_spot_merged= hot_spot_doc1\n",
    "                hot_spot_merged[\"confidence\"] = (hot_spot_doc1[\"confidence\"] + hot_spot_doc2[\"confidence\"])/2\n",
    "                hot_spot_merged[\"surface_temperature_celcius\"] = (hot_spot_doc1[\"surface_temperature_celcius\"] \n",
    "                                                              + hot_spot_doc2[\"surface_temperature_celcius\"])/2\n",
    "                \n",
    "                ####...Embed averaged hotspot record into climate record\n",
    "                document[\"hotspot\"]=hot_spot_merged\n",
    "                ####... save embedded document to database\n",
    "                save_to_db(document)\n",
    "                \n",
    "            ###....Check whether climate location and first one of hotspor location same...    \n",
    "            elif get_geohash(document[\"latitude\"], document[\"longitude\"]\n",
    "                           ,hot_spot_doc1[\"latitude\"], hot_spot_doc1[\"longitude\"]):\n",
    "                \n",
    "                ####...Embed hotspot record into climate record\n",
    "                document[\"hotspot\"] = hot_spot_doc1\n",
    "                ####... save embedded document to database\n",
    "                save_to_db(document)\n",
    "            \n",
    "            ###....Check whether climate location and second one of hotspor location same... \n",
    "            elif get_geohash(document[\"latitude\"], document[\"longitude\"]\n",
    "                           ,hot_spot_doc2[\"latitude\"], hot_spot_doc2[\"longitude\"]):\n",
    "                \n",
    "                ####...Embed hotspot record into climate record\n",
    "                document[\"hotspot\"] = hot_spot_doc2\n",
    "                ####... save embedded document to database\n",
    "                save_to_db(document)\n",
    "            \n",
    "            ###....if no climate location and hotspot location same...\n",
    "            else:\n",
    "                ####... save climate to database\n",
    "                save_to_db(document)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e32c7a2be8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set batch Interval\n",
    "n_secs = 10 \n",
    "# Assign topic names\n",
    "topics = [\"climate_streaming\", \"hotspot_AQUA_streaming\", \"hotspot_TERRA_streaming\"] \n",
    "\n",
    "# Configure and create spark context\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\") \n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, topics, {\n",
    "       'bootstrap.servers': '127.0.0.1:9092',\n",
    "       'group.id': 'assignment',\n",
    "        'fetch.message.max.bytes': '15728640',\n",
    "       'auto.offset.reset': 'largest'})\n",
    "\n",
    "records = kafkaStream.foreachRDD(lambda rdd: join_all_stream(rdd))\n",
    "\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(60) # Run stream for 10 minutes just in case no detection of producer\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
